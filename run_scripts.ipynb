{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Tetris RL script runner\n",
        "\n",
        "Run the baseline policies and the vanilla policy gradient CNN.\n",
        "\n",
        "Note: GUI render modes will open a window. If you are running headless, skip the render commands.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "70739dfe",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Working directory: /Users/colinminini/Desktop/SCOC_ICE/RL_Tetris_Project\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "root = Path.cwd()\n",
        "if (root / 'tetris_code').exists():\n",
        "    pass\n",
        "elif root.name == 'tetris_code' and (root.parent / 'tetris_code').exists():\n",
        "    root = root.parent\n",
        "    os.chdir(root)\n",
        "\n",
        "print('Working directory:', Path.cwd())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fc4563c7",
      "metadata": {},
      "source": [
        "## Baseline policies (render)\n",
        "\n",
        "These open a window to visualize the episode.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "3299b64b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Game Over!\n",
            "Final Score: 11\n"
          ]
        }
      ],
      "source": [
        "!python3 tetris_code/view_episode_policy_random.py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "d6ec8e19",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "^Core: 21\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/colinminini/Desktop/SCOC_ICE/RL_Tetris_Project/tetris_code/view_episode_policy_greedy.py\", line 21, in <module>\n",
            "    action = policies.policy_greedy(env) \n",
            "  File \"/Users/colinminini/Desktop/SCOC_ICE/RL_Tetris_Project/tetris_code/policies.py\", line 96, in policy_greedy\n",
            "    observation, reward, terminated, truncated, info = new_env.step(action)\n",
            "  File \"/Users/colinminini/Desktop/SCOC_ICE/RL_Tetris_Project/.venv/lib/python3.13/site-packages/gymnasium/wrappers/common.py\", line 393, in step\n",
            "    return super().step(action)\n",
            "  File \"/Users/colinminini/Desktop/SCOC_ICE/RL_Tetris_Project/.venv/lib/python3.13/site-packages/gymnasium/core.py\", line 327, in step\n",
            "    return self.env.step(action)\n",
            "  File \"/Users/colinminini/Desktop/SCOC_ICE/RL_Tetris_Project/.venv/lib/python3.13/site-packages/gymnasium/wrappers/common.py\", line 285, in step\n",
            "    return self.env.step(action)\n",
            "  File \"/Users/colinminini/Desktop/SCOC_ICE/RL_Tetris_Project/.venv/lib/python3.13/site-packages/tetris_gymnasium/envs/tetris.py\", line 267, in step\n",
            "    self._get_obs(),\n",
            "  File \"/Users/colinminini/Desktop/SCOC_ICE/RL_Tetris_Project/.venv/lib/python3.13/site-packages/tetris_gymnasium/envs/tetris.py\", line 600, in _get_obs\n",
            "    t = copy.deepcopy(self.tetrominoes[t_id])\n",
            "  File \"/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/copy.py\", line 163, in deepcopy\n",
            "    y = _reconstruct(x, memo, *rv)\n",
            "  File \"/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/copy.py\", line 269, in _reconstruct\n",
            "    y.__dict__.update(state)\n",
            "KeyboardInterrupt\n"
          ]
        }
      ],
      "source": [
        "!python3 tetris_code/view_episode_policy_greedy.py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "97c2862b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Game Over!\n",
            "Final Score: 10\n"
          ]
        }
      ],
      "source": [
        "!python3 tetris_code/view_episode_policy_down.py\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "150882f7",
      "metadata": {},
      "source": [
        "## Train the vanilla policy gradient CNN\n",
        "\n",
        "Adjust `--episodes` and `--lr` as needed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08854df4",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 10/500 avg_reward=11.50 loss=-8.1618\n",
            "Episode 20/500 avg_reward=12.00 loss=-3.4895\n",
            "Episode 30/500 avg_reward=9.80 loss=12.9351\n",
            "Episode 40/500 avg_reward=9.40 loss=-2.1371\n",
            "Episode 50/500 avg_reward=9.40 loss=-2.6470\n",
            "Episode 60/500 avg_reward=9.40 loss=9.7559\n",
            "Episode 70/500 avg_reward=9.70 loss=-3.5654\n",
            "Episode 80/500 avg_reward=9.70 loss=-5.8657\n",
            "Episode 90/500 avg_reward=9.80 loss=-5.4945\n",
            "Episode 100/500 avg_reward=9.80 loss=-3.3098\n",
            "Episode 110/500 avg_reward=9.70 loss=-3.6400\n",
            "Episode 120/500 avg_reward=9.90 loss=1.7315\n",
            "Episode 130/500 avg_reward=9.70 loss=3.3003\n",
            "Episode 140/500 avg_reward=9.60 loss=-1.6163\n",
            "Episode 150/500 avg_reward=10.20 loss=-0.3797\n",
            "Episode 160/500 avg_reward=10.00 loss=-0.1061\n",
            "Episode 170/500 avg_reward=9.80 loss=-1.2423\n",
            "Episode 180/500 avg_reward=9.90 loss=-2.4446\n",
            "Episode 190/500 avg_reward=10.00 loss=1.9912\n",
            "Episode 200/500 avg_reward=10.10 loss=4.5046\n",
            "Episode 210/500 avg_reward=9.90 loss=3.6949\n",
            "Episode 220/500 avg_reward=9.70 loss=-1.9723\n",
            "Episode 230/500 avg_reward=10.20 loss=-2.0371\n",
            "Episode 240/500 avg_reward=10.10 loss=-0.6552\n",
            "Episode 250/500 avg_reward=10.10 loss=2.1207\n",
            "Episode 260/500 avg_reward=9.80 loss=-0.3514\n",
            "Episode 270/500 avg_reward=9.80 loss=-5.3942\n",
            "Episode 280/500 avg_reward=9.60 loss=-1.1476\n",
            "Episode 290/500 avg_reward=9.50 loss=0.7266\n",
            "Episode 300/500 avg_reward=9.80 loss=2.9672\n",
            "Episode 310/500 avg_reward=9.80 loss=-2.3355\n",
            "Episode 320/500 avg_reward=9.80 loss=2.0418\n",
            "Episode 330/500 avg_reward=9.80 loss=-2.3285\n",
            "Episode 340/500 avg_reward=9.90 loss=-5.1754\n",
            "Episode 350/500 avg_reward=9.60 loss=-0.2070\n",
            "Episode 360/500 avg_reward=9.80 loss=-5.9757\n",
            "Episode 370/500 avg_reward=9.90 loss=-7.0790\n",
            "Episode 380/500 avg_reward=9.60 loss=-6.8216\n",
            "Episode 390/500 avg_reward=9.80 loss=-0.0709\n",
            "Episode 400/500 avg_reward=10.10 loss=-2.8612\n",
            "Episode 410/500 avg_reward=9.90 loss=-6.2774\n",
            "Episode 420/500 avg_reward=10.20 loss=-12.1015\n",
            "Episode 430/500 avg_reward=10.40 loss=-13.6749\n",
            "Episode 440/500 avg_reward=10.10 loss=-7.1182\n",
            "Episode 450/500 avg_reward=10.30 loss=-8.5299\n",
            "Episode 460/500 avg_reward=10.50 loss=-9.0028\n",
            "Episode 470/500 avg_reward=9.80 loss=-10.0921\n",
            "Episode 480/500 avg_reward=10.30 loss=-4.5548\n",
            "Episode 490/500 avg_reward=10.10 loss=-4.1664\n",
            "Episode 500/500 avg_reward=10.00 loss=-4.6810\n",
            "Saved checkpoint to tetris_code/checkpoints/pg_cnn.pt\n"
          ]
        }
      ],
      "source": [
        "!python3 tetris_code/train_pg_cnn.py --episodes 100 --normalize-returns --save-path tetris_code/checkpoints/pg_cnn.pt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5aa0b3fd",
      "metadata": {},
      "source": [
        "## Evaluate the trained policy\n",
        "\n",
        "Use `--render` for a visual run, or `--stochastic` to sample actions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "b35d2a52",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 1 reward=10.00\n",
            "Episode 2 reward=9.00\n",
            "Episode 3 reward=10.00\n",
            "Episode 4 reward=9.00\n",
            "Episode 5 reward=9.00\n",
            "Episode 6 reward=9.00\n",
            "Episode 7 reward=10.00\n",
            "Episode 8 reward=9.00\n",
            "Episode 9 reward=10.00\n",
            "Episode 10 reward=11.00\n",
            "Average reward=9.60 +/- 0.66 (n=10)\n"
          ]
        }
      ],
      "source": [
        "!python3 tetris_code/evaluate_pg_cnn.py --episodes 10 --model-path tetris_code/checkpoints/pg_cnn.pt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dbd7e68f",
      "metadata": {},
      "source": [
        "## Train DQN with after-states\n",
        "\n",
        "This uses macro-actions (rotate/move + hard drop) and after-state values.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "c0be9797",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 10/50 avg_reward=-134.84 epsilon=0.993\n",
            "Episode 20/50 avg_reward=-117.62 epsilon=0.988\n",
            "Episode 30/50 avg_reward=-129.24 epsilon=0.981\n",
            "Episode 40/50 avg_reward=-134.78 epsilon=0.974\n",
            "Episode 50/50 avg_reward=-142.54 epsilon=0.968\n",
            "Saved checkpoint to tetris_code/checkpoints/dqn_afterstate.pt\n"
          ]
        }
      ],
      "source": [
        "!python3 tetris_code/train_dqn_afterstate.py --episodes 50 --save-path tetris_code/checkpoints/dqn_afterstate.pt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ebef601",
      "metadata": {},
      "source": [
        "## Evaluate the DQN after-state agent\n",
        "\n",
        "Use `--render` for a visual run.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "9c797eea",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "^Cisode number 1 Score: 1802.0\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/colinminini/Desktop/SCOC_ICE/Reinforcement Learning/tetris_code/evaluate_dqn_afterstate.py\", line 168, in <module>\n",
            "    main()\n",
            "  File \"/Users/colinminini/Desktop/SCOC_ICE/Reinforcement Learning/tetris_code/evaluate_dqn_afterstate.py\", line 153, in main\n",
            "    candidates = enumerate_after_states(env, sequences)\n",
            "  File \"/Users/colinminini/Desktop/SCOC_ICE/Reinforcement Learning/tetris_code/dqn_afterstate.py\", line 173, in enumerate_after_states\n",
            "    obs, _, done, info = simulate_sequence(env, seq)\n",
            "  File \"/Users/colinminini/Desktop/SCOC_ICE/Reinforcement Learning/tetris_code/dqn_afterstate.py\", line 139, in simulate_sequence\n",
            "    obs, reward, terminated, truncated, info = new_env.step(action)\n",
            "  File \"/Users/colinminini/Desktop/SCOC_ICE/Reinforcement Learning/.venv/lib/python3.13/site-packages/gymnasium/wrappers/common.py\", line 393, in step\n",
            "    return super().step(action)\n",
            "  File \"/Users/colinminini/Desktop/SCOC_ICE/Reinforcement Learning/.venv/lib/python3.13/site-packages/gymnasium/core.py\", line 327, in step\n",
            "    return self.env.step(action)\n",
            "  File \"/Users/colinminini/Desktop/SCOC_ICE/Reinforcement Learning/.venv/lib/python3.13/site-packages/gymnasium/wrappers/common.py\", line 285, in step\n",
            "    return self.env.step(action)\n",
            "  File \"/Users/colinminini/Desktop/SCOC_ICE/Reinforcement Learning/.venv/lib/python3.13/site-packages/tetris_gymnasium/envs/tetris.py\", line 267, in step\n",
            "    self._get_obs(),\n",
            "  File \"/Users/colinminini/Desktop/SCOC_ICE/Reinforcement Learning/.venv/lib/python3.13/site-packages/tetris_gymnasium/envs/tetris.py\", line 601, in _get_obs\n",
            "    t.matrix = np.pad(\n",
            "  File \"/Users/colinminini/Desktop/SCOC_ICE/Reinforcement Learning/.venv/lib/python3.13/site-packages/numpy/lib/arraypad.py\", line 786, in pad\n",
            "    unsupported_kwargs = set(kwargs) - set(allowed_kwargs[mode])\n",
            "KeyboardInterrupt\n"
          ]
        }
      ],
      "source": [
        "!python3 tetris_code/evaluate_dqn_afterstate.py --episodes 10 --model-path tetris_code/checkpoints/dqn_afterstate.pt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a70ba03",
      "metadata": {},
      "source": [
        "## Render a DQN episode\n",
        "\n",
        "This will open a window to visualize the trained agent.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "85cb2168",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "^Cisode number 1 Score: 2069.0\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/colinminini/Desktop/SCOC_ICE/Reinforcement Learning/tetris_code/evaluate_dqn_afterstate.py\", line 168, in <module>\n",
            "    main()\n",
            "  File \"/Users/colinminini/Desktop/SCOC_ICE/Reinforcement Learning/tetris_code/evaluate_dqn_afterstate.py\", line 130, in main\n",
            "    obs, reward_sum, done, info = run_sequence(\n",
            "  File \"/Users/colinminini/Desktop/SCOC_ICE/Reinforcement Learning/tetris_code/dqn_afterstate.py\", line 157, in run_sequence\n",
            "    render_fn(env)\n",
            "  File \"/Users/colinminini/Desktop/SCOC_ICE/Reinforcement Learning/tetris_code/evaluate_dqn_afterstate.py\", line 77, in render_step\n",
            "    env.render()\n",
            "  File \"/Users/colinminini/Desktop/SCOC_ICE/Reinforcement Learning/.venv/lib/python3.13/site-packages/gymnasium/wrappers/common.py\", line 409, in render\n",
            "    return super().render()\n",
            "  File \"/Users/colinminini/Desktop/SCOC_ICE/Reinforcement Learning/.venv/lib/python3.13/site-packages/gymnasium/core.py\", line 337, in render\n",
            "    return self.env.render()\n",
            "  File \"/Users/colinminini/Desktop/SCOC_ICE/Reinforcement Learning/.venv/lib/python3.13/site-packages/gymnasium/wrappers/common.py\", line 303, in render\n",
            "    return self.env.render()\n",
            "  File \"/Users/colinminini/Desktop/SCOC_ICE/Reinforcement Learning/.venv/lib/python3.13/site-packages/tetris_gymnasium/envs/tetris.py\", line 371, in render\n",
            "    matrix = np.kron(matrix, kernel)\n",
            "  File \"/Users/colinminini/Desktop/SCOC_ICE/Reinforcement Learning/.venv/lib/python3.13/site-packages/numpy/lib/shape_base.py\", line 1173, in kron\n",
            "    result = _nx.multiply(a_arr, b_arr, subok=(not is_any_mat))\n",
            "KeyboardInterrupt\n"
          ]
        }
      ],
      "source": [
        "!python3 tetris_code/evaluate_dqn_afterstate.py --episodes 1 --model-path tetris_code/checkpoints/dqn_afterstate.pt --render\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "193256d5",
      "metadata": {},
      "source": [
        "## Render a single evaluation episode\n",
        "\n",
        "This will open a window to visualize the trained policy.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42d73afe",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 1 reward=11.00\n",
            "Episode 2 reward=10.00\n",
            "Episode 3 reward=10.00\n",
            "Episode 4 reward=10.00\n",
            "Episode 5 reward=10.00\n",
            "Episode 6 reward=10.00\n",
            "Episode 7 reward=10.00\n",
            "Episode 8 reward=10.00\n",
            "Episode 9 reward=11.00\n",
            "Episode 10 reward=10.00\n",
            "Average reward=10.20 +/- 0.40 (n=10)\n"
          ]
        }
      ],
      "source": [
        "!python3 tetris_code/evaluate_pg_cnn.py --episodes 1 --model-path tetris_code/checkpoints/pg_cnn.pt --render\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
