Questions to Prepare: Tetris Environment

- Why do we learn after-state Value and not Q? Is it because the entropy of the environment is 0? p(s'/s,a) = 1 or 0
- How do we define after-states here? How do you compute legal states possible from s?
- Do you compute them after knowing the current piece and the current state of the board (s)?
- Why do we transform state into features?
- Why use Neural Network for approximating Q (or V)? Why MLP on these features instead of CNN on the 20 x 10 x 1 state?
- What is the dimension of the states s?
- How many actions can you take? What is the tuple of action?
- How do you take actions during eval? 
- When you know which legal s' from s has the highest V(s'): How do you go from s to s'? Is it a sequence of move and rotation?
- How do we score our agent? What is the reward system? Score == Return of the episode?
- Why do we shape the reward function this way?
- What is the loss function? What do we optimize? J(theta)?