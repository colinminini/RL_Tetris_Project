Questions to Prepare: Tetris Environment

- Why do we learn after-state Value and not Q? Is it because the entropy of the environment is 0? p(s'/s,a) = 1 or 0
- How do we define after-states here? How do you compute legal states possible from s?
- Do you compute them after knowing the current piece and the current state of the board (s)?
- Why do we transform state into features?
- Why not using Iterative Q-Learning instead of NN Loss Optimization to find Q*?
- Why MLP on these heandcrafted features instead of CNN on the 20 x 10 x 1 state?
- What is the dimension of the states s?
- What Double means in DDQN?
- How many actions can you take? What is a macro-action?
- What is your policy after training is done - during eval?
- When you know which legal s' from s has the highest V(s'): How do you go from s to s'?
- How do we score our agent? What is the reward system? Score == Return of the episode?
- Why do we shape the reward function this way?
- What is the loss function? What do we optimize? J(theta)?